behaviors:
  My Behavior:
    trainer_type: ppo
    hyperparameters:
      batch_size: 4096                # Larger batch size for stable updates
      buffer_size: 40960              # Larger buffer size for more experience
      learning_rate: 0.0005          # Moderate learning rate for stability
      beta: 0.002                    # Lower entropy coefficient for less exploration
      epsilon: 0.2                   # Larger epsilon for more exploration
      lambd: 0.95                    # Higher lambda for better advantage smoothing
      num_epoch: 3                   # Fewer epochs per update for faster convergence
      learning_rate_schedule: linear # Gradually decaying learning rate
    network_settings:
      normalize: true                 # Keep normalization for faster convergence
      hidden_units: 512               # Larger hidden layer for more expressive power
      num_layers: 3                   # Additional layer for more model capacity
      vis_encode_type: simple         # Simple encoding for visual inputs
    reward_signals:
      extrinsic:
        gamma: 0.99                   # High gamma for long-term rewards
        strength: 1.0                  # Full strength of extrinsic rewards
    keep_checkpoints: 5
    max_steps: 10000000               # Higher number of steps for better training
    time_horizon: 1000
    summary_freq: 1000
    self_play:
      window: 10
      play_against_latest_model_ratio: 0.5
      save_steps: 5000
      team_change: 3000
      swap_steps: 1000
